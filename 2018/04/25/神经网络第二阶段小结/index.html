<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Neural Networks," />










<meta name="description" content="在陆续完成Prof. Andrew Ng的第二门和第三门深度学习课程后，除了继续领略和折服于深度学习的神秘和优雅之外，好像更多的还是深深感到其雄才伟略未能食之冰山一角！DNN的特性犹如大脑中无数的神经元，使得机器也具备了认知复杂事物的能力（复杂事物的源分布），真正无监督时代的到来将意味着机器真正对人类的超越！历史总在不经意间泛起波澜，谁又能知道下一页会如何书写呢？回归正题，由于时间有限，本文简单介">
<meta name="keywords" content="Neural Networks">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络第二阶段小结">
<meta property="og:url" content="http://www.caomi.xyz/2018/04/25/神经网络第二阶段小结/index.html">
<meta property="og:site_name" content="Forsaken">
<meta property="og:description" content="在陆续完成Prof. Andrew Ng的第二门和第三门深度学习课程后，除了继续领略和折服于深度学习的神秘和优雅之外，好像更多的还是深深感到其雄才伟略未能食之冰山一角！DNN的特性犹如大脑中无数的神经元，使得机器也具备了认知复杂事物的能力（复杂事物的源分布），真正无监督时代的到来将意味着机器真正对人类的超越！历史总在不经意间泛起波澜，谁又能知道下一页会如何书写呢？回归正题，由于时间有限，本文简单介">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://www.caomi.xyz/2018/04/25/神经网络第二阶段小结/tanh.jpg">
<meta property="og:updated_time" content="2018-04-26T14:26:29.336Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络第二阶段小结">
<meta name="twitter:description" content="在陆续完成Prof. Andrew Ng的第二门和第三门深度学习课程后，除了继续领略和折服于深度学习的神秘和优雅之外，好像更多的还是深深感到其雄才伟略未能食之冰山一角！DNN的特性犹如大脑中无数的神经元，使得机器也具备了认知复杂事物的能力（复杂事物的源分布），真正无监督时代的到来将意味着机器真正对人类的超越！历史总在不经意间泛起波澜，谁又能知道下一页会如何书写呢？回归正题，由于时间有限，本文简单介">
<meta name="twitter:image" content="http://www.caomi.xyz/2018/04/25/神经网络第二阶段小结/tanh.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.caomi.xyz/2018/04/25/神经网络第二阶段小结/"/>





  <title>神经网络第二阶段小结 | Forsaken</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Forsaken</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">My feet have never returned to the ground.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-categories" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.caomi.xyz/2018/04/25/神经网络第二阶段小结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Forsaken">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Forsaken">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">神经网络第二阶段小结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-25T20:35:27+08:00">
                2018-04-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithm/" itemprop="url" rel="index">
                    <span itemprop="name">algorithm</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>在陆续完成Prof. Andrew Ng的第二门和第三门深度学习课程后，除了继续领略和折服于深度学习的神秘和优雅之外，好像更多的还是深深感到其雄才伟略未能食之冰山一角！DNN的特性犹如大脑中无数的神经元，使得机器也具备了认知复杂事物的能力（复杂事物的源分布），真正无监督时代的到来将意味着机器真正对人类的超越！历史总在不经意间泛起波澜，谁又能知道下一页会如何书写呢？回归正题，由于时间有限，本文简单介绍一下DNN的利器：BN.<a id="more"></a></p>
<h1 id="浅谈Batch-Normalization-BN算法"><a href="#浅谈Batch-Normalization-BN算法" class="headerlink" title="浅谈Batch Normalization(BN算法)"></a>浅谈Batch Normalization(BN算法)</h1><p>Batch Normalization(批标准化)为Google2015年发表于计算机类顶级会议ICML上的一篇文章，号称当年机器学习界最有影响力的文章，其原名为：《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》，下面先叙述该算法，<br>对\(z^{(i)}\)进行操作：<br>$$\mu = \frac{1}{m}\sum_{i}z^{(i)} \cdots (1) $$<br>$$\sigma^2 = \frac{1}{m}\sum_{i}(z^{(i)} - \mu)^{2} \cdots (2) $$<br>$$z_{norm}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2+\varepsilon}} \cdots (3)(加\varepsilon是为了避免除0) $$<br>$$\overset{\sim}{z}^{(i)} = \gamma z_{norm}^{(i)}+\beta \cdots (4)(\gamma和\beta的值可通过学习得到)$$</p>
<p>上述\( (1)～(3)\)为标准化步骤，\( (4)\)为反标准化步骤。<br>$$E(z_{norm}^{(i)}) = 0,\quad D(z_{norm}^{(i)}) = 1,\quad E(\overset{\sim}{z}^{(i)}) = \beta,\quad D(\overset{\sim}{z}^{(i)}) = \gamma^2.$$<br>注意：如果\(\gamma = \sqrt{\sigma^2+\varepsilon}, \beta = \mu \Rightarrow \overset{\sim}{z}^{(i)} = z^{(i)}. \)即将BN处理后的\( \overset{\sim}{z}^{(i)} \)可以还原为\( z^{(i)} \)</p>
<h2 id="1、在MBGD上应用BN："><a href="#1、在MBGD上应用BN：" class="headerlink" title="1、在MBGD上应用BN："></a>1、在MBGD上应用BN：</h2><p>$$X^{\lbrace 1 \rbrace} \xrightarrow{W^{[1]}, b{[1]}}Z^{[1]} \xrightarrow{\beta^{[1]}, \gamma^{[1]}} \overset{\sim}{Z}^{[1]} \to g^{(1)}(\overset{\sim}{Z}^{[1]}) = A^{[1]} \xrightarrow{W^{[2]}, b^{[2]}} Z^{[2]} \to \cdots$$<br>$$X^{\lbrace 2\rbrace } \xrightarrow{W^{[1]}, b^{[1]}}Z^{[1]} \xrightarrow[BN]{\beta^{[1]}, \gamma^{[1]}} \overset{\sim}{Z}^{[1]} \to \cdots $$<br>$$X^{\lbrace3\rbrace} \to \cdots $$<br>$$\cdots $$<br>parameters:\( W^{[l]}, b^{[l]}, \beta^{[l]}, \gamma^{[l]} \)，可忽略参数\( b^{[l]} \)<br>原因：\( z^{[l]} = w^{[l]}a^{[l-1]}+b^{[l]} \)，因为计算了\( z^{[l]} \)的均值，因此\( b^{[l]} \)常数项可直接忽略. \( \Rightarrow z^{[l]} = w^{[l]}\cdot a^{[l-1]} \Rightarrow \overset{\sim}{z}^{[l]} = \gamma^{[l]} \cdot z^{[l]}+\beta^{[l]} \).实际上\( b^{[l]} \)将被\( \beta^{[l]} \)替代，并由\( \beta^{[l]} \)作用于\( \overset{\sim}{z}^{[l]} \)并传递到下一层。</p>
<h2 id="2、BN算法实现："><a href="#2、BN算法实现：" class="headerlink" title="2、BN算法实现："></a>2、BN算法实现：</h2><p>for \( t = 1, \cdots ,\)num-Mini-batches:<br>\( \quad\quad \)compute forward prop on \( X^{\lbrace t \rbrace} \) in each hidden layer, use BN to replace \( z^{[l]} \) with \( \overset{\sim}{z}^{[l]} \)<br>\( \quad\quad \)update parameters:<br>    $$w^{[l]}:= w^{[l]}-\alpha\cdot w^{[l]}$$<br>    $$\beta^{[l]}:= \beta^{[l]} - \alpha\cdot \beta^{[l]}$$<br>    $$\gamma^{[l]}:= \gamma^{[l]}- \alpha\cdot \gamma^{[l]}$$<br>MGD，RMSprop，Adam同样奏效于\( w^{[l]}, \beta^{[l]}, \gamma^{[l]} \)的更新。</p>
<h2 id="3、BN算法起作用的原因："><a href="#3、BN算法起作用的原因：" class="headerlink" title="3、BN算法起作用的原因："></a>3、BN算法起作用的原因：</h2><p>从数据分布，即解决“covariate shift”（协变量）的角度解释：<br>解决训练过程中出现的“covariate shift”（输入数据的分布产生变化）问题，在DNN的训练过程中，由于每次迭代都会更新各层的参数w和b，且通过激活函数进入下一层，即：<br>$$z^{[l]} = w^{[l]}a^{[l-1]}+b^{[l]} \Rightarrow g^{[l]}(z^{[l]}) = a^{[l]} \Rightarrow z^{[l+1]} = w^{[l+1]}a^{[l]}+b^{[l+1]} $$<br>（1）\( z^{[l+1]} \)的分布会显著异于\( z^{[l]} \)的分布，且在每次迭代之后，\( z^{[l+1]} \)的分布也会异于之前的\( z^{[l+1]} \)的分布，因为前面层的w和b在不断更新；<br>（2）\( z^{[l]} \)经过\( g^{[l]} \)（非线性）处理后得到的\( a^{[l]} \)也会与“源分布”X显著相异<br>综上，就产生了“covariate shift”的问题。<br>训练DNN的本质就是为了找到数据集的真实分布，而如果DNN在训练过程中，其隐藏层的数据分布在每轮迭代后都会发生显著变化的话，那么隐藏层中学习的参数实际上会不断的去适应/学习新的数据分布，这样显然就会使得学习的效率变低，而在应用了BN之后，将各隐藏层的\( z^{[l]} \)统一标准化为均值为0，方差为1的分布，使其不会因为前面层参数w、b的更新和非线性激活函数的影响而导致每轮迭代后\( z^{[l]} \)的数据分布都发生大的变化，这样就有效解决了“covariate shift”的问题，即降低了隐藏单元\( z^{[l]} \)分布的不稳定性，使得\( z^{[l]} \)的值变得更稳定，对前面层参数的依赖性降低（不会使\( z^{[l]} \)的分布完全不变，因为\( \beta^{[l]} \)和\( \gamma^{[l]} \)也是不断更新的），在降低了“covariate shift”问题之后，也就是说降低了后面层参数和前面层参数的耦合，这样就能使得网络的每一层都能独立学习，这样就能有效提升整个网络的学习速度和robust性。<br>而至于为什么要引入反标准化参数\( \gamma \)和\( \beta \)的问题，我认为原因有三：<br>（1）在旧参数中，\( z^{[l]} \)的均值和方差取决于上层网络的复杂关联，但在新参数中，仅有\( \gamma \)和\( \beta \)来确定，去除了与上层计算的密切耦合，从而能让每一层都能独立的去学习源分布（对\( z^{[l]} \)进行规范化能达到让各隐藏层都独立学习源分布（\( a^{[l]}, X \)）的效果，因为z会作为输入进入激活函数，输出学习到的数据分布），这样每一层都能独立的向源分布（\( a^{[l]}, X \)）靠拢，这样最后学到的模型就一定是最接近源分布的模型；<br>（2）规范化操作会将几乎所有的数据都映射到激活函数的非饱和区（线性区），（针对tanh和sigmoid），这样就降低了神经网络的表达能力，而进行再变换，则可以将数据从线性区变换到非线性区，这样就能起到恢复模型表达能力的效果；<br>（3）为了让BN后的数据分布具备恢复到未经BN处理的数据分布的能力。<br>感性理解：<br>让神经网络自己学着使用和修改\( \gamma \)和\( \beta \)，这样NN就能自己琢磨出前面的normalization有没有起到优化的作用，如果没有，就用\( \gamma \)和\( \beta \)来抵消一些normalization的操作。同时，经过BN处理后的数据分布会更加均匀，使cost更加对称，就如同对原始数据进行归一化的效果一样，这样不仅能使用更大的学习率，同时还能保证在训练过程中下降的方向不出现大的变化，这样也显然能加快学习的速度。</p>
<h2 id="4、关于BN能解决梯度弥散的问题："><a href="#4、关于BN能解决梯度弥散的问题：" class="headerlink" title="4、关于BN能解决梯度弥散的问题："></a>4、关于BN能解决梯度弥散的问题：</h2><p>即不会让每层的输入过大或者过小，这样才能发挥（tanh和sigmoid）这类非线性激活函数的非线性拟合能力。<br><img src="/2018/04/25/神经网络第二阶段小结/tanh.jpg" alt="tanh"><br>由于对\( z^{[l]} \)做了标准化处理，因此不会让每层的输入过大或者过小，而在BP过程中，若\( z^{[l]} \)过大或者过小，会使得\( g\prime (z^{[l]}) \)变得很小，这样就会产生梯度弥散问题，如：<br>$$dz^{[1]} = \frac{dL}{dz^{[1]}} = \frac{dL}{da^{[3]}}\cdot \frac{da^{[3]}}{dz^{[3]}}\cdot \frac{dz^{[3]}}{da^{[2]}}\cdot \frac{da^{[2]}}{dz^{[2]}}\cdot \frac{dz^{[2]}}{da^{[1]}}\cdot \frac{da^{[1]}}{dz^{[1]}}$$<br>其中\( \frac{da^{[3]}}{dz^{[3]}} = g^{(3)^{\prime}}(z^{[3]}),\frac{da^{[2]}}{dz^{[2]}} = g^{(2)^{\prime}}(z^{[2]}), \frac{da^{[1]}}{dz^{[1]}} = g^{(1)^{\prime}}(z^{[1]}) \)，这样，在DNN中就极其容易出现指数级梯度弥散，而如果做了规范化处理，即将右边红色区域映射到了左边红色区域，这样就能在BP过程中防止梯度弥散（从w的角度在防止梯度弥散或梯度爆炸是在初始化上做文章来防止），同时，由于\( \gamma \)和\( \beta \)参数的引入，则能将归一化后的数据从线性区重新变换到非线形区，这样就能恢复模型的表达能力。这实际上是从梯度弥散的角度解释了BN加快DNN的学习速度的原因（梯度小的话学习速度就会非常慢），但如果是使用ReLU的话，实际上并不会产生这个问题，但是“covariate shift”问题依然存在，所以BN依然是有效的，相当于是对tanh或sigmoid的一个副作用。</p>
<h2 id="5、BN算法产生噪声的原因："><a href="#5、BN算法产生噪声的原因：" class="headerlink" title="5、BN算法产生噪声的原因："></a>5、BN算法产生噪声的原因：</h2><p>因为是在mini-batch上计算的均值和方差，而不是在整个训练集上计算的，这样就难免使得BN后的值，即\( \overset{\sim}{z}^{[l]} \)在该mini-batch上产生一定的噪声（如果是在整个训练集上做规范化，则不会新增加噪声）。直观理解就是：<br>$$0.5 \quad 0.7 \quad 0.6 \Rightarrow 1 (\text{其原始标签为1})$$<br>而在另一个mini-batch中规范化后得到同样的特征：<br>$$0.5 \quad 0.7 \quad 0.6 \Rightarrow 0 (\text{其原始标签为0})$$<br>这样用\( \overset{\sim}{z}^{(i)} \)计算\( a^{(i)} \)时肯定也会含有噪声。（因为带有噪声，就会使得后续的隐藏单元不过多的依赖前面隐藏单元（类dropout，产生的噪声不会比dropout多，因此其正则化效果是远远赶不上dropout的），每一层的独立性更强，也不愿把太多的权重放在一个隐藏单元上。）</p>
<h2 id="6、Batch-Norm-at-test-time-（通过训练集来估算测试集的-mu-和-sigma-2-）"><a href="#6、Batch-Norm-at-test-time-（通过训练集来估算测试集的-mu-和-sigma-2-）" class="headerlink" title="6、Batch Norm at test time:（通过训练集来估算测试集的\( \mu \)和\( \sigma^2 \)）"></a>6、Batch Norm at test time:（通过训练集来估算测试集的\( \mu \)和\( \sigma^2 \)）</h2><p>\(\mu,\sigma^2 \) estimate using EWMA across mini-batches，<br>$$V_t = \beta V_{t-1}+(1-\beta)\theta_t $$<br>在测试时，使用上面计算出的\( \mu \)和\( \sigma^2 \)来进行缩放，即：<br>$$ z_{norm}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2+\varepsilon}} \Rightarrow \overset{\sim}{z}^{(i)} = \gamma z_{norm}^{(i)}+\beta \Rightarrow \hat{y} = g(\overset{\sim}{z}^{(i)}) $$</p>
<h2 id="7、在Tensorflow中实现BN："><a href="#7、在Tensorflow中实现BN：" class="headerlink" title="7、在Tensorflow中实现BN："></a>7、在Tensorflow中实现BN：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">tf.nn.batch_normalization()#一句代码即可实现</span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift[C]. ICML, 2015.</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Forsaken 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Neural-Networks/" rel="tag"># Neural Networks</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/23/神经网络第一阶段小结/" rel="next" title="神经网络第一阶段小结">
                <i class="fa fa-chevron-left"></i> 神经网络第一阶段小结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/06/浅谈ResNet/" rel="prev" title="浅谈ResNet">
                浅谈ResNet <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNTE1Ny8xMTY5Mg=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.gif"
                alt="Forsaken" />
            
              <p class="site-author-name" itemprop="name">Forsaken</p>
              <p class="site-description motion-element" itemprop="description">Land of the free.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/cmiforsaken" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#浅谈Batch-Normalization-BN算法"><span class="nav-number">1.</span> <span class="nav-text">浅谈Batch Normalization(BN算法)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1、在MBGD上应用BN："><span class="nav-number">1.1.</span> <span class="nav-text">1、在MBGD上应用BN：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2、BN算法实现："><span class="nav-number">1.2.</span> <span class="nav-text">2、BN算法实现：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3、BN算法起作用的原因："><span class="nav-number">1.3.</span> <span class="nav-text">3、BN算法起作用的原因：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4、关于BN能解决梯度弥散的问题："><span class="nav-number">1.4.</span> <span class="nav-text">4、关于BN能解决梯度弥散的问题：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5、BN算法产生噪声的原因："><span class="nav-number">1.5.</span> <span class="nav-text">5、BN算法产生噪声的原因：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6、Batch-Norm-at-test-time-（通过训练集来估算测试集的-mu-和-sigma-2-）"><span class="nav-number">1.6.</span> <span class="nav-text">6、Batch Norm at test time:（通过训练集来估算测试集的\( \mu \)和\( \sigma^2 \)）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7、在Tensorflow中实现BN："><span class="nav-number">1.7.</span> <span class="nav-text">7、在Tensorflow中实现BN：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">1.8.</span> <span class="nav-text">参考文献</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Forsaken</span>

  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a></div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
